{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12608740,"sourceType":"datasetVersion","datasetId":7964582}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ishaks2005/airfare-forecasting-system?scriptVersionId=253139364\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# ‚úàÔ∏è Flight Price Prediction\nThis notebook builds and tests several models to predict flight prices based on various features like airline, source, duration, stops, and more.\n\n## üîç Objective\nTo predict the price of a flight using machine learning techniques on the dataset flight_data.\n","metadata":{}},{"cell_type":"markdown","source":"## 1. üì¶ Importing Libraries\nWe begin by importing essential libraries for data manipulation, visualization, and modeling.\n","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler,OneHotEncoder,OrdinalEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split,RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression,Ridge,SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor,ExtraTreesRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom lightgbm import LGBMRegressor\nfrom scipy.stats import randint, uniform","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. üóÇÔ∏è Loading Data\nWe load the data and take an initial look.","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/flight-data/flight_data.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. üìä Exploratory Data Analysis (EDA)\n\nIn this section, we explore the dataset to understand its structure, uncover patterns, and identify any issues such as missing values, duplicates, or outliers.\n","metadata":{}},{"cell_type":"markdown","source":"### üìê  Shape and Structure of Data\n\n- **Data Shape**: (4000 rows √ó 12 columns)\n  \n\nThis gives us an idea of dataset size and dimensionality.","metadata":{}},{"cell_type":"code","source":"#Shape of data\n\ndata_shape = data.shape\nprint(f'Shape of training data : {data_shape}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üßæ  Column Names and Data Types\n\nWe inspect the column names and check data types to identify categorical vs. numerical features. This also helps in planning preprocessing and encoding steps.\n\n* Numerical columns: price, duration, days_left, flight_freq\n\n* Categorical columns : stops, class, departure, arrival, airline, source, destination, flight\n","metadata":{}},{"cell_type":"code","source":"#Information on data and column datatypes\n\ndata_info = data.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  üìä   Descriptive Statistics\n\nUsing `.describe()`, we look at:\n- Mean, Count, Std, min, max, and percentiles of numerical features like `id`,`price`,`duration`, and `days_left`.The median values are shown in the 50% row.\n- Presence of outliers (e.g., unusually high `price`)","metadata":{}},{"cell_type":"code","source":"#Descriptive Statistics\n\ndata.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ‚ùó Missing Values\n\nWe check for missing values using `.isnull().sum()`. Handling these is essential before modeling, especially for features like `days_left`.\n","metadata":{}},{"cell_type":"code","source":"missing_values = data.isnull().sum()\nprint(f'Number of missing values corresponding to each column :\\n {missing_values}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üîÅ Duplicate Records\n\nWe find and count duplicate rows **excluding the `id` column**, which is unique by design.\n\n- Number of duplicated rows found: **250**\n\nDuplicates can distort model training, so we drop them to ensure clean learning.\n","metadata":{}},{"cell_type":"code","source":"number_of_duplicates = data.duplicated(subset=[col for col in data.columns if col != 'id']).sum()\nprint(f'Number of Duplicated Rows : {number_of_duplicates}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üì¶ Outlier Detection Using Boxplots\n\nTo visually identify potential outliers in the dataset, we plotted boxplots for all numeric columns:\n\n‚úÖ Columns Visualized:\n\n   `id`, `duration`, `days_left`, `price`\n\nüí° **Insights:**\n\n1. `id`:\n\n* The id column appears uniformly distributed with no significant outliers.\n\n* Since id is likely just a unique identifier, it's not useful for modeling and can be dropped.\n\n2. `duration`:\n\n* Shows a large number of high-end outliers.\n\n* Some flight durations are significantly longer than the majority, which may be genuine long-haul flights or possible data entry errors.\n\n\n3. `days_left`:\n\n* The spread is reasonable with no clear outliers.\n\n* Distribution seems fairly even ‚Äî this feature is clean and useful for modeling.\n\n4. `price`:\n\n* The price column shows a clear right-skew with many outliers.\n\n* These high-priced tickets could heavily influence model performance.","metadata":{}},{"cell_type":"code","source":"#Boxplot\n\nnumeric_cols = data.select_dtypes(include=np.number).columns\n\nplt.figure(figsize=(15, 10))\n\nfor i, col in enumerate(numeric_cols, 1):\n    plt.subplot(len(numeric_cols) // 3 + 1, 3, i)\n    sns.boxplot(x=data[col])\n    plt.title(f'Boxplot of {col}')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n### üå°Ô∏è Correlation Heatmap\n\nWe compute correlations between numerical features using `.corr()`.\n\nüí° **Insights:**\n \n1. **`duration` vs `price` ‚Üí Positive Correlation (0.22):**  \n   - As flight duration increases, the price tends to increase.  \n   - This aligns with intuition ‚Äî longer flights are generally more expensive due to fuel, service, and distance.\n\n2. **`days_left` vs `price` ‚Üí Negative Correlation (-0.093):**  \n   - As the number of days left before departure increases, price tends to decrease.  \n   - This supports real-world behavior: booking earlier usually results in cheaper fares.\n","metadata":{}},{"cell_type":"code","source":"#Heatmap\n\nnumeric_cols = data.select_dtypes(include=np.number)\ncorr_matrix = numeric_cols.corr()\nsns.heatmap(corr_matrix,cmap='Reds',annot=True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üßÆ Target Distribution (Histogram)\n\nWe visualize the distribution of `price`, the target variable.\n\nüí° **Insights**:\n\n* The `price` distribution is **right-skewed**, with most flights priced below ‚Çπ10,000.\n* There are several high-priced outliers (business class or last-minute bookings).\n","metadata":{}},{"cell_type":"code","source":"#Histogram\n\nplt.figure(figsize=(10,5))\nsns.histplot(data['price'], bins=30, kde=True)\nplt.title(\"Original Target Distribution\")\nplt.xlabel(\"Target\")\nplt.ylabel(\"Frequency\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. üßπ Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### ‚úÖ Dropping Irrelevant Columns\n\n* `id` column was dropped from both train and test sets as it provides no predictive value.","metadata":{}},{"cell_type":"code","source":"#Dropping irrelevant columns\n\ndata = data.drop('id',axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üîÅ Duplicate Rows\n\n* Although duplicate rows were detected in the dataset, they were intentionally not dropped. After experimentation, it was found that retaining the duplicates improved model performance (as measured by validation metrics).\n\n* Hence, they were preserved to leverage potentially valuable signal in repeated patterns.","metadata":{}},{"cell_type":"code","source":"'''#Removing Duplicates\n\ndef remove_duplicates(data):\n    cols_to_check = [col for col in data.columns]\n    data_cleaned = data.drop_duplicates(subset=cols_to_check).reset_index(drop=True)\n    return data_cleaned\n\nprint(f\"Train data shape before removing duplicates : {data.shape}\")\n\ntrain_data = remove_duplicates(data)\n\nprint(f\"Data shape after removing duplicates: {data.shape}\")'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ‚úÖ Adjusting flight Column\n\n* Some flight values were in scientific notation ‚Äî these were replaced with NaN for consistency.","metadata":{}},{"cell_type":"code","source":"#Adjusting flight column(removing scientific notations)\n\ndata['flight'] = data['flight'].replace(to_replace=r'^\\d+\\.\\d+[eE][+-]?\\d+$', value=np.nan, regex=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üìà Outlier Handling\n\n* We capped outliers in price and duration using the IQR method:\n* Outliers were replaced with these bounds to minimize distortion while preserving overall data distribution.","metadata":{}},{"cell_type":"code","source":"#Capping Outliers\n\ndef get_iqr_bounds(series):\n    Q1 = series.quantile(0.25)\n    Q3 = series.quantile(0.75)\n    IQR = Q3 - Q1\n    lower = Q1 - 1.5 * IQR\n    upper = Q3 + 1.5 * IQR\n    return lower, upper\n\ndef cap_column(series, lower, upper):\n    return np.where(series < lower, lower,\n                    np.where(series > upper, upper, series))\n\ncolumns_with_outliers = ['price', 'duration']  \n\nfor col in columns_with_outliers:\n    lower, upper = get_iqr_bounds(data[col])\n    data[col] = cap_column(data[col], lower, upper)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üéØ Feature & Target Separation \n\nThe target variable for this task is `price`. All other relevant columns were selected as features.","metadata":{}},{"cell_type":"code","source":"# Feature-Target Separation\n\nX_train_data = data.drop('price',axis=1)\ny_train_data = data['price']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ü™ì Train-Validation Split\n\nThe dataset was split into training, validation and test sets to evaluate model generalization.","metadata":{}},{"cell_type":"code","source":"#Train-Test-Validation Split\n\n# First split: Train (70%) and Temp (30%)\nX_train, X_temp, y_train, y_temp = train_test_split(X_train_data, y_train_data, test_size=0.3, random_state=42)\n\n# Second split: Validation (15%) and Test (15%)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üî¢ Feature Engineering\n\n* The flight column contains a large number of unique values, making it a high-cardinality categorical feature. Traditional one-hot encoding would result in a very sparse and high-dimensional dataset, which can negatively impact model performance and training time.\n\n* To address this, we applied frequency encoding, where each flight is replaced by the number of times it appears in the dataset.","metadata":{}},{"cell_type":"code","source":"#Frequency Encoding\n\nflight_freq = X_train['flight'].value_counts(normalize=True)\n\nfor df in [X_train,X_val,X_test]:\n    df['flight_freq'] = df['flight'].map(flight_freq).fillna(0)\n    df.drop('flight', axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üîÑ Preprocessing Pipeline\n\nTo prepare the dataset for modeling, we implemented customized preprocessing pipelines tailored to the nature of each feature and the requirements of different algorithms. This section outlines how we handled missing values, encoded categorical variables, and scaled features.\n\n* Airline:\n\n    * Used `SimpleImputer(strategy='most_frequent')` because airline names are categorical and often repeated.\n\n   *  Applied `OrdinalEncoder()` to convert airline names to numeric form suitable for models without inflating feature space.\n\n* Numerical Columns (duration, days_left, flight_freq):\n\n    * Used `median` strategy to handle skewed numerical distributions robustly.\n  \n    * Applied `StandardScaler()` (in scaled pipeline) to normalize values for models sensitive to scale (e.g., linear models).\n\n* Ordinal Columns (stops, class, arrival, departure):\n\n    * Used `most_frequent` as these are categorical and missingness often implies repetition.\n\n    * Used `OrdinalEncoder` with domain-specific category order (e.g., class: Economy < Business) to preserve ordinal relationship.\n\n* One-Hot Columns (source, destination):\n\n    * Used `most_frequent` due to high likelihood of repetition in categories. \n\n    * Used `OneHotEncoder(drop='first')` to avoid dummy variable trap and handle categorical data without implied order\n\n\n* ColumnTransformer \n\n    Combined all pipelines into a unified `ColumnTransformer` to preprocess different column types in parallel.\n\n   - `preprocessor_non_scaled`: For models that don‚Äôt require feature scaling.\n      \n   - `preprocessor_scaled`: For models that benefit from scaled numerical inputs (e.g., linear regreesion).","metadata":{}},{"cell_type":"code","source":"#Pipeline\n\nnum_missing_cols = ['duration','days_left','flight_freq']\none_hot_cols = ['source','destination']\nordinal_cols = ['stops','class','arrival','departure']\nall_numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\nordinal_map = [\n    ['zero', 'one', 'two_or_more'],  \n    ['Economy', 'Business'],        \n    ['Early_Morning', 'Morning', 'Afternoon', 'Evening', 'Night', 'Late_Night'],  # for 'departure'\n    ['Early_Morning', 'Morning', 'Afternoon', 'Evening', 'Night', 'Late_Night']   # for 'arrival'\n]\n\nairline_pipeline = Pipeline([\n    ('airline_imputer', SimpleImputer(strategy='most_frequent')),\n    ('label_encoder', OrdinalEncoder())\n])\n\nnum_pipeline = Pipeline([\n    ('num_imputer',SimpleImputer(strategy='median'))\n])\n\nnum_pipeline_scaled = Pipeline([\n    ('num_imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\nordinal_pipeline = Pipeline([\n    ('ordinal_impute',SimpleImputer(strategy='most_frequent')),\n    ('ordinal_encode',OrdinalEncoder(categories=ordinal_map))\n])\n\none_hot_pipeline = Pipeline([\n    ('one_hot_impute',SimpleImputer(strategy='most_frequent')),\n    ('one_hot_encode',OneHotEncoder(drop='first', handle_unknown='ignore'))\n])\n\npreprocessor_non_scaled = ColumnTransformer([\n    ('airline_label', airline_pipeline, ['airline']),\n    ('numerical',num_pipeline,num_missing_cols),\n    ('ordinal_encoding',ordinal_pipeline,ordinal_cols),\n    ('one-hot-encoding',one_hot_pipeline,one_hot_cols)\n], remainder='passthrough')\n\npreprocessor_scaled = ColumnTransformer([\n    ('airline_label', airline_pipeline, ['airline']),\n    ('numerical', num_pipeline_scaled, all_numeric_cols),  \n    ('ordinal_encoding', ordinal_pipeline, ordinal_cols),\n    ('one-hot-encoding', one_hot_pipeline, one_hot_cols)\n], remainder='passthrough')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.üß† Model Building, Tuning, and Evaluation","metadata":{}},{"cell_type":"markdown","source":"### üèóÔ∏è  Model Building\n- A total of **8 different regression models** were trained on the data:\n  - Linear Regression\n  - Extra Trees Regressor\n  - SGD Regressor  \n  - Random Forest  \n  - Gradient Boosting  \n  - XGBoost  \n  - LightGBM  \n  - K-Nearest Neighbors (KNN)\n\nEach model was wrapped in a pipeline that included preprocessing specific to its requirements (scaled vs. non-scaled).\n","metadata":{}},{"cell_type":"code","source":"#Defining Models\n\nmodels = {\n    \"Linear Regression\": LinearRegression(),\n    \"SGD Regression\": SGDRegressor(max_iter=1000, tol=1e-3, random_state=42),\n    \"Random Forest\": RandomForestRegressor(random_state=42),\n    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n    \"XGBoost\": XGBRegressor(random_state=42, verbosity=0),\n    \"LightGBM\": LGBMRegressor(random_state=42,verbose=-1),\n    \"KNN Regression\": KNeighborsRegressor(),\n    \"Extra Trees\": ExtraTreesRegressor(random_state=42),\n    \n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üéØ  Hyperparameter Tuning\n- **RandomizedSearchCV** was used to tune hyperparameters for the following 3 models:\n  \n  - **Random** **Forest**: Tuned `n_estimators` and `max_depth`\n     \n  - **XGBoost**: Tuned `n_estimators`, `max_depth`, `learning_rate`, `subsample`, and `colsample_bytree`\n    \n  - **LightGBM**: Tuned the same hyperparameters as XGBoost for fairness in comparison","metadata":{}},{"cell_type":"code","source":"#Hyperparameter Tuning\n\nparam_distributions = {\n    \"Random Forest\": {\n        \"model__n_estimators\": randint(50, 200),\n        \"model__max_depth\": randint(3, 20)\n    },\n    \"XGBoost\": {\n        \"model__n_estimators\": randint(50, 200),\n        \"model__max_depth\": randint(3, 20),\n        \"model__learning_rate\": uniform(0.01, 0.3),\n        \"model__subsample\": uniform(0.6, 0.4),           \n        \"model__colsample_bytree\": uniform(0.6, 0.4)\n    },\n    \"LightGBM\": {\n        \"model__n_estimators\": randint(50, 200),\n        \"model__max_depth\": randint(3, 20),\n        \"model__learning_rate\": uniform(0.01, 0.3),\n        \"model__subsample\": uniform(0.6, 0.4),           \n        \"model__colsample_bytree\": uniform(0.6, 0.4)\n    }\n}\n\n# Only tune these models\nmodels_to_tune = [\"Random Forest\", \"XGBoost\", \"LightGBM\"]\n\nfor name in models_to_tune:\n    print(f\"Tuning {name}...\")\n    \n    preprocessor = preprocessor_non_scaled  \n\n    pipeline = Pipeline([\n        ('preprocess', preprocessor),\n        ('model', models[name])\n    ])\n    \n    search = RandomizedSearchCV(\n        pipeline,\n        param_distributions[name],\n        n_iter=10,\n        cv=3,\n        scoring='neg_root_mean_squared_error',\n        verbose=0,\n        random_state=42,\n        n_jobs=1\n    )\n    \n    search.fit(X_train, np.log1p(y_train))\n    best_pipeline = search.best_estimator_\n    \n    # Update model with best model for use in evaluation\n    models[name] = best_pipeline.named_steps['model']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üìä  Model Evaluation\n\n- All trained models were evaluated on the validation set using the following metrics:\n  \n  - **R¬≤ Score**\n    \n  - **Root Mean Squared Error (RMSE)**\n \n    \n  - **Mean Absolute Error (MAE)**\n\nPredictions were log-transformed during training and inverse-transformed before evaluation.\n\nThe results were compiled in a dataframe and sorted by **R¬≤ Score** for comparison.","metadata":{}},{"cell_type":"code","source":"#Training and Prediction on Evaluation set\n\nresults = []\n\ntree_models = ['Random Forest', 'Gradient Boosting', 'XGBoost', 'LightGBM','Extra Trees']\n\nfor name, model in models.items():\n    if name in tree_models:\n        preprocessor = preprocessor_non_scaled\n    else:\n        preprocessor = preprocessor_scaled\n\n    full_pipeline = Pipeline([\n        ('preprocess', preprocessor),\n        ('model', model)\n    ])\n\n    full_pipeline.fit(X_train, np.log1p(y_train))\n    preds_log = full_pipeline.predict(X_val)\n\n    # Inverse log transform\n    preds = np.expm1(preds_log)\n\n    results.append({\n        \"Model\": name,\n        \"R2 Score\": r2_score(y_val, preds),\n        \"RMSE\": np.sqrt(mean_squared_error(y_val, preds)),\n        \"MAE\": mean_absolute_error(y_val, preds)\n    })\n\nresults_df = pd.DataFrame(results).sort_values(by=\"R2 Score\", ascending=False)\ndisplay(results_df.reset_index(drop=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### üèÅ  Final Model Selection and Submission\n\n- The **best-performing model** on the validation set was selected based on R¬≤ score.\n  \n- This model was retrained on the combined training and validation set (`X_full`, `y_full`).\n\n- Final predictions were made on the test set.\n","metadata":{}},{"cell_type":"code","source":"# Combining train set and val set\nX_full = pd.concat([X_train, X_val], axis=0).reset_index(drop=True)\ny_full = pd.concat([y_train, y_val], axis=0).reset_index(drop=True)\n\n\n#Using best model for test prediction\nfinal_results = []\n\nbest_model_name = results_df.iloc[0]['Model']\nbest_model = models[best_model_name]\nprint(\"Best Model:\",best_model_name)\n\nif best_model_name in ('Linear Regression','SGD Regression','KNN Regression'):\n    final_pipeline = Pipeline([\n        ('preprocess', preprocessor_scaled),\n        ('model', best_model)\n    ])\nelse:\n    final_pipeline = Pipeline([\n        ('preprocess', preprocessor_non_scaled),\n        ('model', best_model)\n    ])\n    \nfinal_pipeline.fit(X_full, np.log1p(y_full))\ny_test_pred_log = final_pipeline.predict(X_test)\ny_test_pred = np.expm1(y_test_pred_log)\n\nfinal_results.append({\n        \"Model\": best_model_name,\n        \"R2 Score\": r2_score(y_test, y_test_pred),\n        \"RMSE\": np.sqrt(mean_squared_error(y_test, y_test_pred)),\n        \"MAE\": mean_absolute_error(y_test, y_test_pred)\n })\n\nfinal_results_df = pd.DataFrame(final_results)\ndisplay(final_results_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}